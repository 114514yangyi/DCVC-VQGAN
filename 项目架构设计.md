# DCVC 训练代码改造项目架构设计

## 项目概述

本项目旨在对 DCVC 训练代码进行两项主要改造：

1. **数据集改造**：将数据集从图片序列（PNG）改为直接支持 MP4 视频文件
2. **I-frame 模型替换**：将 CompressAI I-frame 模型替换为 VQGAN 模型

---

## 任务一：数据集改造 - 支持 MP4 视频文件

### 1.1 目标

- 修改 `DCVC-family/DCVC/dataset.py`，使其支持直接读取 MP4 视频文件
- 参考 `DCVC-family/DCVC/vqgan/project/train/video_utils.py` 中的 `VideoDataset` 实现
- 在训练代码中可以直接通过参数指定训练集和验证集目录路径

### 1.2 当前状态分析

#### 当前数据集实现（dataset.py）

**VimeoGOPDataset**:
- 从图片序列（PNG）读取数据
- 需要 `septuplet_list.txt` 文件指定序列路径
- 从 7 帧图片中随机选择 GOP 大小的连续帧

**BVI_AOM_Dataset**:
- 从目录中的图片文件读取
- 需要每个序列一个目录，目录中包含 PNG/JPG 文件

**HEVCB_Dataset**:
- 用于评估，从图片序列读取

#### 参考实现（video_utils.py）

**VideoDataset**:
- 使用 OpenCV (`cv2.VideoCapture`) 直接读取 MP4 文件
- 支持随机选择起始帧
- 只读取需要的帧数，不加载整个视频（内存优化）
- 支持目标分辨率调整
- 返回格式：`(video, video_path)`，其中 video 是 `(sequence_length, height, width, channels)` 的 numpy 数组，uint8 类型

### 1.3 详细修改计划

#### 步骤 1.1：创建新的 VideoDataset 类

**文件**：`DCVC-family/DCVC/dataset.py`

**修改内容**：

1. **添加必要的导入**：
```python
import cv2  # 新增：用于读取视频文件
```

2. **创建 VideoGOPDataset 类**（参考 video_utils.py 的 VideoDataset）：
```python
class VideoGOPDataset(Dataset):
    """
    基于 MP4 视频文件的 GOP 数据集
    支持直接从视频文件中读取指定 GOP 大小的帧序列
    """
    def __init__(self, video_dir, gop_size=7, transform=None, crop_size=256, 
                 random_shuffle=True, seed=None, max_frames_to_check=1000):
        """
        参数:
            video_dir: 视频文件目录路径（包含 .mp4 文件）
            gop_size: GOP 大小（帧数）
            transform: 图像变换（PIL Image -> Tensor）
            crop_size: 随机裁剪大小
            random_shuffle: 是否随机选择起始帧
            seed: 随机种子
            max_frames_to_check: 检查视频总帧数时的最大限制
        """
        # 实现细节见下方
```

**实现要点**：
- 扫描 `video_dir` 目录，找到所有 `.mp4` 文件
- 使用 `cv2.VideoCapture` 读取视频
- 随机选择起始帧（如果 `random_shuffle=True`）
- 只读取 `gop_size` 帧，不加载整个视频
- 支持随机裁剪（如果 `crop_size` 指定）
- 将 BGR 转换为 RGB（OpenCV 默认 BGR）
- 转换为 PIL Image 或直接转换为 Tensor（根据 transform）
- 返回格式：`torch.Tensor`，形状为 `(gop_size, C, H, W)`，值范围 [0, 1]

#### 步骤 1.2：创建 VideoValidationDataset 类

**文件**：`DCVC-family/DCVC/dataset.py`

**修改内容**：

创建用于验证的数据集类，支持从 MP4 文件读取完整视频序列：

```python
class VideoValidationDataset(Dataset):
    """
    用于验证的 MP4 视频数据集
    读取完整视频序列用于评估
    """
    def __init__(self, video_dir, transform=None, max_frames=None):
        """
        参数:
            video_dir: 视频文件目录路径
            transform: 图像变换
            max_frames: 最大读取帧数（None 表示读取全部）
        """
        # 实现细节
```

**实现要点**：
- 读取完整的视频序列（或最多 `max_frames` 帧）
- 返回格式：字典，包含 `{'frames': tensor, 'name': video_name, 'num_frames': int}`

#### 步骤 1.3：修改训练代码参数

**文件**：`DCVC-family/DCVC/train_dcvc.py`

**修改位置**：`setup_dataset` 函数和参数解析部分

**修改内容**：

1. **添加新的命令行参数**：
```python
# 在 argparse 中添加
parser.add_argument('--train_video_dir', type=str, default=None,
                    help='Path to training video directory (MP4 files)')
parser.add_argument('--val_video_dir', type=str, default=None,
                    help='Path to validation video directory (MP4 files)')
parser.add_argument('--use_video_dataset', action='store_true',
                    help='Use MP4 video dataset instead of image sequences')
```

2. **修改 `setup_dataset` 函数**：
```python
def setup_dataset(args, config):
    """Setup dataset based on stage and data type."""
    if config['is_finetuning']:
        # BVI-AOM 数据集逻辑（保持不变或也支持视频）
        ...
    else:
        # 主训练数据集
        if args.use_video_dataset:
            # 使用新的 VideoGOPDataset
            if not args.train_video_dir:
                raise ValueError("--train_video_dir must be provided when using video dataset")
            
            train_transform = transforms.ToTensor()
            dataset = VideoGOPDataset(
                video_dir=args.train_video_dir,
                gop_size=config['gop_size'],
                transform=train_transform,
                crop_size=args.crop_size,
                random_shuffle=True,
                seed=args.seed
            )
        else:
            # 原有的 Vimeo-90k 数据集逻辑（保持向后兼容）
            ...
```

3. **修改评估数据集设置**：
```python
# 在 evaluate_hevc_b 函数或相关位置
if args.use_video_dataset and args.val_video_dir:
    eval_dataset = VideoValidationDataset(
        video_dir=args.val_video_dir,
        transform=transforms.ToTensor(),
        max_frames=args.eval_max_frames
    )
else:
    # 原有的 HEVCB_Dataset 逻辑
    eval_dataset = HEVCB_Dataset(...)
```

#### 步骤 1.4：数据格式转换处理

**关键点**：确保数据格式与现有训练代码兼容

**当前训练代码期望的格式**：
- `train_one_epoch` 函数中：`gop_batch` 的形状是 `(batch_size, gop_size, C, H, W)`
- 值范围：[0, 1]（float32）

**VideoDataset 返回格式**：
- `(gop_size, H, W, C)` numpy array，uint8，值范围 [0, 255]

**需要转换**：
- 在 `VideoGOPDataset.__getitem__` 中：
  1. 将 BGR 转换为 RGB
  2. 转换为 PIL Image（如果需要应用 transform）
  3. 应用 transform（ToTensor 会将 [0, 255] 转换为 [0, 1]）
  4. 返回 `(gop_size, C, H, W)` 的 Tensor

#### 步骤 1.5：保持向后兼容性

**策略**：
- 保留原有的 `VimeoGOPDataset`、`BVI_AOM_Dataset`、`HEVCB_Dataset` 类
- 通过 `--use_video_dataset` 标志选择使用哪种数据集
- 如果未指定 `--use_video_dataset`，使用原有逻辑

---

## 任务二：I-frame 模型替换 - CompressAI → VQGAN

### 2.1 目标

- 将训练代码中的 CompressAI I-frame 模型替换为 VQGAN 模型
- 保持 I-frame 压缩接口的一致性，确保训练流程不受影响
- 支持 VQGAN 模型的加载、编码、解码和 BPP 计算

### 2.2 当前状态分析

#### 当前 I-frame 实现（train_dcvc.py）

**加载方式**：
```python
from compressai.zoo import models as compressai_models
i_frame_model = compressai_models[args.i_frame_model_name](
    quality=args.i_frame_quality,
    pretrained=args.i_frame_pretrained
).to(device)
```

**使用方式**：
```python
def compress_i_frame_with_padding(i_frame_model, frame_tensor, calculate_bpp=True):
    # 压缩 I-frame
    encoded = i_frame_model(rgb_padded)
    x_hat_rgb = encoded['x_hat']  # 重建的 RGB 图像
    # 计算 BPP
    for likelihood in encoded['likelihoods'].values():
        bits = torch.log(likelihood).sum() / (-math.log(2))
    bpp = total_bits.item() / num_pixels
    return encoded, bpp
```

**接口特点**：
- 输入：`[B, C, H, W]` Tensor，值范围 [0, 1]
- 输出：字典，包含 `'x_hat'`（重建图像）和 `'likelihoods'`（用于 BPP 计算）
- 支持 padding（16 的倍数）

#### VQGAN 模型特点

**加载方式**（参考 recon_judge.py）：
```python
def load_vqgan_model(vq_config: str, vq_ckpt: str, device: str):
    with open(vq_config, 'r') as f:
        config = json.load(f)
    model_args = config['model_args']
    model = create_model(model_args=model_args, config_path=vq_config)
    model = model.to(device)
    checkpoint = torch.load(vq_ckpt, map_location=device)
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()
    return model, config
```

**使用方式**：
```python
# 编码
indices = vqgan_model.encode(frame_tensor)  # 返回 codebook indices

# 解码
decoded = vqgan_model.decode(indices)  # 返回重建图像
```

**接口特点**：
- 输入：需要归一化到 [-1, 1]（mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]）
- 输出：重建图像，也需要反归一化
- 没有直接的 BPP 计算接口（需要额外实现）

### 2.3 详细修改计划

#### 步骤 2.1：创建 VQGAN I-frame 包装器

**文件**：`DCVC-family/DCVC/src/models/vqgan_iframe.py`（新建）

**目的**：创建一个包装类，使 VQGAN 模型的接口与 CompressAI 模型兼容

**实现**：

```python
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import json
import math

class VQGANIFrameModel(nn.Module):
    """
    VQGAN I-frame 模型包装器
    提供与 CompressAI I-frame 模型兼容的接口
    """
    def __init__(self, vq_config_path, vq_checkpoint_path, device):
        super().__init__()
        self.device = device
        self.vqgan_model = self._load_vqgan_model(vq_config_path, vq_checkpoint_path, device)
        self.vqgan_model.eval()
        
        # 归一化和反归一化
        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        self.unnormalize = transforms.Normalize(
            mean=[-1.0, -1.0, -1.0],
            std=[2.0, 2.0, 2.0]
        )
        
        # 获取 codebook 大小（用于 BPP 计算）
        self.codebook_size = self._get_codebook_size()
    
    def _load_vqgan_model(self, config_path, checkpoint_path, device):
        """加载 VQGAN 模型"""
        # 实现加载逻辑（参考 recon_judge.py）
        ...
    
    def _get_codebook_size(self):
        """获取 codebook 大小"""
        # 从模型配置或模型中获取
        ...
    
    def forward(self, x):
        """
        前向传播，兼容 CompressAI 接口
        
        输入:
            x: [B, C, H, W] Tensor，值范围 [0, 1]
        
        输出:
            dict: {
                'x_hat': 重建图像 [B, C, H, W]，值范围 [0, 1]
                'likelihoods': 用于 BPP 计算的似然值（字典）
            }
        """
        # 1. 归一化到 [-1, 1]
        x_normalized = self.normalize(x)
        
        # 2. 编码
        with torch.no_grad():
            indices = self.vqgan_model.encode(x_normalized)
        
        # 3. 解码
        with torch.no_grad():
            x_hat_normalized = self.vqgan_model.decode(indices)
        
        # 4. 反归一化到 [0, 1]
        x_hat = self.unnormalize(x_hat_normalized)
        x_hat = torch.clamp(x_hat, 0, 1)
        
        # 5. 计算 BPP（使用均匀分布假设或实际熵编码）
        likelihoods = self._calculate_likelihoods(indices, x.shape)
        
        return {
            'x_hat': x_hat,
            'likelihoods': likelihoods,
            'indices': indices  # 可选：保存 indices 用于调试
        }
    
    def _calculate_likelihoods(self, indices, input_shape):
        """
        计算 codebook indices 的似然值
        
        注意：VQGAN 使用离散 codebook，需要额外的熵编码模型
        这里可以使用简单的均匀分布假设，或加载训练好的熵编码模型
        """
        B, C, H, W = input_shape
        num_pixels = B * H * W
        
        # 方法1：均匀分布假设（简单但不准确）
        # 每个 index 的概率 = 1 / codebook_size
        uniform_prob = 1.0 / self.codebook_size
        # 创建与 indices 形状相同的概率张量
        likelihood = torch.full_like(indices, uniform_prob, dtype=torch.float32)
        
        # 方法2：使用实际熵编码模型（如果可用）
        # 这里需要额外的 Transformer 或其他熵编码模型
        
        return {'y': likelihood}
    
    def compress(self, x):
        """
        压缩接口（可选，用于真实压缩）
        """
        # 实现压缩逻辑
        ...
    
    def decompress(self, strings):
        """
        解压缩接口（可选，用于真实解压缩）
        """
        # 实现解压缩逻辑
        ...
```

#### 步骤 2.2：实现 BPP 计算

**关键问题**：VQGAN 使用离散 codebook，需要额外的熵编码模型来计算准确的 BPP

**方案选择**：

**方案 A：简单均匀分布假设**
- 假设每个 codebook index 出现的概率相等
- BPP = log2(codebook_size) / (H * W)
- 简单但不准确

**方案 B：使用训练好的熵编码模型**
- 加载 Transformer 或其他熵编码模型
- 对 codebook indices 序列进行熵编码
- 计算实际比特率
- 准确但需要额外的模型

**推荐实现**（分阶段）：
1. 第一阶段：使用方案 A（快速实现）
2. 第二阶段：集成熵编码模型（如果需要准确的 BPP）

#### 步骤 2.3：修改训练代码中的 I-frame 加载

**文件**：`DCVC-family/DCVC/train_dcvc.py`

**修改位置**：`main()` 函数中加载 I-frame 模型的部分

**修改内容**：

```python
# 在 main() 函数中

# 添加参数
parser.add_argument('--i_frame_type', type=str, default='compressai',
                    choices=['compressai', 'vqgan'],
                    help='I-frame model type: compressai or vqgan')
parser.add_argument('--vqgan_config', type=str, default=None,
                    help='VQGAN config file path (required if i_frame_type=vqgan)')
parser.add_argument('--vqgan_checkpoint', type=str, default=None,
                    help='VQGAN checkpoint path (required if i_frame_type=vqgan)')

# 修改加载逻辑
if args.i_frame_type == 'vqgan':
    if not args.vqgan_config or not args.vqgan_checkpoint:
        raise ValueError("--vqgan_config and --vqgan_checkpoint must be provided when using VQGAN")
    
    from src.models.vqgan_iframe import VQGANIFrameModel
    i_frame_model = VQGANIFrameModel(
        vq_config_path=args.vqgan_config,
        vq_checkpoint_path=args.vqgan_checkpoint,
        device=device
    )
    i_frame_model.eval()
    
    if accelerator.is_main_process:
        logger.info(f"Loaded VQGAN I-frame model")
        logger.info(f"Config: {args.vqgan_config}")
        logger.info(f"Checkpoint: {args.vqgan_checkpoint}")
else:
    # 原有的 CompressAI 加载逻辑
    i_frame_model = compressai_models[args.i_frame_model_name](
        quality=args.i_frame_quality,
        pretrained=args.i_frame_pretrained
    ).to(device)
    i_frame_model.eval()
```

#### 步骤 2.4：确保接口兼容性

**关键点**：`compress_i_frame_with_padding` 函数需要兼容两种模型

**修改策略**：
- 保持 `compress_i_frame_with_padding` 函数接口不变
- VQGAN 包装器返回的字典格式与 CompressAI 兼容
- 确保 `'x_hat'` 和 `'likelihoods'` 键存在

**修改内容**：

```python
def compress_i_frame_with_padding(i_frame_model, frame_tensor, calculate_bpp=True):
    """
    兼容 CompressAI 和 VQGAN 的 I-frame 压缩函数
    """
    B, C, H, W = frame_tensor.shape
    
    # Padding 逻辑（保持不变）
    padding_r, padding_b = H % 16, W % 16
    if padding_r > 0 or padding_b > 0:
        rgb_padded = torch.nn.functional.pad(frame_tensor, (0, padding_b, 0, padding_r), mode='reflect')
    else:
        rgb_padded = frame_tensor
    
    # 调用模型（接口已统一）
    encoded = i_frame_model(rgb_padded)
    
    # 提取重建图像
    x_hat_rgb = encoded['x_hat']
    
    # 移除 padding
    if padding_r > 0 or padding_b > 0:
        x_hat_rgb = x_hat_rgb[:, :, :H, :W]
    
    encoded['x_hat'] = x_hat_rgb
    
    # 计算 BPP（如果请求）
    if calculate_bpp:
        num_pixels = B * H * W
        total_bits = 0.0
        for likelihood in encoded['likelihoods'].values():
            bits = torch.log(likelihood).sum() / (-math.log(2))
            total_bits += bits
        bpp = total_bits.item() / num_pixels
        return encoded, bpp
    else:
        return encoded
```

#### 步骤 2.5：处理 VQGAN 模型依赖

**需要导入的模块**：
- VQGAN 模型创建函数（`create_model`）
- 可能需要从 `vqgan/project` 导入相关模块

**路径处理**：
- 确保可以正确导入 VQGAN 相关模块
- 可能需要添加路径到 `sys.path`

**实现**：

```python
# 在 vqgan_iframe.py 中
import sys
import os

# 添加 VQGAN 项目路径
_vqgan_project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__), '../../vqgan/project')
)
if _vqgan_project_root not in sys.path:
    sys.path.insert(0, _vqgan_project_root)

# 导入 VQGAN 相关模块
from models.taming.models.vqgan import VQModel, EMAVQ
# 或使用 create_model 函数（如果存在）
```

---

## 实施步骤总结

### 阶段一：数据集改造

1. ✅ **步骤 1.1**：在 `dataset.py` 中创建 `VideoGOPDataset` 类
2. ✅ **步骤 1.2**：在 `dataset.py` 中创建 `VideoValidationDataset` 类
3. ✅ **步骤 1.3**：修改 `train_dcvc.py` 的参数解析和 `setup_dataset` 函数
4. ✅ **步骤 1.4**：确保数据格式转换正确
5. ✅ **步骤 1.5**：测试向后兼容性

### 阶段二：I-frame 模型替换

1. ✅ **步骤 2.1**：创建 `src/models/vqgan_iframe.py` 文件，实现 `VQGANIFrameModel` 类
2. ✅ **步骤 2.2**：实现 BPP 计算逻辑（先使用简单方案）
3. ✅ **步骤 2.3**：修改 `train_dcvc.py` 中的 I-frame 加载逻辑
4. ✅ **步骤 2.4**：确保 `compress_i_frame_with_padding` 函数兼容
5. ✅ **步骤 2.5**：处理 VQGAN 模型依赖和路径问题

### 阶段三：测试和验证

1. ✅ **单元测试**：测试新的数据集类
2. ✅ **集成测试**：测试 VQGAN I-frame 模型加载和使用
3. ✅ **训练测试**：运行一个简短的训练周期，验证整个流程
4. ✅ **性能对比**：对比 CompressAI 和 VQGAN 的 I-frame 压缩效果

---

## 注意事项

### 数据格式一致性

- **输入格式**：确保两种数据集返回的数据格式一致
  - 形状：`(gop_size, C, H, W)`
  - 值范围：[0, 1]（float32）
  - 数据类型：`torch.Tensor`

### 内存优化

- **视频读取**：只读取需要的帧数，不加载整个视频
- **及时释放**：读取完帧后立即释放 `cv2.VideoCapture` 对象
- **数据类型**：在数据加载阶段使用 uint8，在模型输入时转换为 float32

### 向后兼容性

- **保留原有类**：不删除原有的数据集类
- **参数选择**：通过命令行参数选择使用哪种数据集和 I-frame 模型
- **默认行为**：如果不指定新参数，使用原有逻辑

### 错误处理

- **文件检查**：检查视频文件是否存在、可读
- **格式验证**：验证视频格式是否支持
- **异常处理**：视频读取失败时直接抛出异常，中断处理（不自动跳过）
  - 无法打开视频文件：抛出 `RuntimeError`
  - 视频帧数不足：抛出 `ValueError`
  - 读取帧失败：抛出 `RuntimeError`
  - 格式转换失败：抛出 `RuntimeError`

### 性能考虑

- **数据加载**：使用多线程数据加载（`num_workers`）
- **批处理**：确保批处理大小合适
- **GPU 内存**：注意 VQGAN 模型的内存占用

---

## 文件修改清单

### 需要修改的文件

1. **DCVC-family/DCVC/dataset.py**
   - 添加 `VideoGOPDataset` 类
   - 添加 `VideoValidationDataset` 类
   - 添加必要的导入（cv2）

2. **DCVC-family/DCVC/train_dcvc.py**
   - 添加视频数据集相关参数
   - 修改 `setup_dataset` 函数
   - 修改 I-frame 模型加载逻辑
   - 添加 VQGAN 相关参数

### 需要新建的文件

1. **DCVC-family/DCVC/src/models/vqgan_iframe.py**
   - 实现 `VQGANIFrameModel` 类
   - 实现 VQGAN 模型加载和包装逻辑

### 可能需要修改的文件

1. **DCVC-family/DCVC/test_video.py**（如果也需要支持视频数据集）
2. **DCVC-family/DCVC/test_iframe.py**（如果也需要支持 VQGAN I-frame）

---

## 测试计划

### 测试用例 1：视频数据集加载

```python
# 测试 VideoGOPDataset
dataset = VideoGOPDataset(
    video_dir="/path/to/videos",
    gop_size=7,
    crop_size=256
)
sample = dataset[0]
assert sample.shape == (7, 3, 256, 256)
assert sample.min() >= 0 and sample.max() <= 1
```

### 测试用例 2：VQGAN I-frame 模型

```python
# 测试 VQGANIFrameModel
model = VQGANIFrameModel(
    vq_config_path="/path/to/config.json",
    vq_checkpoint_path="/path/to/checkpoint.pth",
    device="cuda:0"
)
test_input = torch.rand(1, 3, 256, 256)
output = model(test_input)
assert 'x_hat' in output
assert 'likelihoods' in output
assert output['x_hat'].shape == test_input.shape
```

### 测试用例 3：端到端训练

```bash
# 使用视频数据集和 VQGAN I-frame 进行训练
python train_dcvc.py \
  --use_video_dataset \
  --train_video_dir /path/to/train/videos \
  --val_video_dir /path/to/val/videos \
  --i_frame_type vqgan \
  --vqgan_config /path/to/vqgan_config.json \
  --vqgan_checkpoint /path/to/vqgan_checkpoint.pth \
  --stage 1 \
  --gop_size 2 \
  --epochs 1
```

---

## 完成标准

### 功能完成标准

- [ ] 可以使用 `--train_video_dir` 和 `--val_video_dir` 参数指定视频数据集
- [ ] 可以使用 `--i_frame_type vqgan` 和 VQGAN 相关参数使用 VQGAN I-frame 模型
- [ ] 训练代码可以正常运行，不报错
- [ ] 数据格式与原有代码兼容
- [ ] 保持向后兼容性（原有参数仍然可用）

### 质量完成标准

- [ ] 代码通过基本测试
- [ ] 代码有适当的注释和文档
- [ ] 错误处理完善
- [ ] 性能可接受（数据加载速度、内存占用）

---

## 后续优化方向

1. **BPP 计算优化**：集成实际的熵编码模型，提高 BPP 计算准确性
2. **数据增强**：为视频数据集添加更多数据增强选项
3. **多视频支持**：支持从多个目录加载视频
4. **视频格式支持**：支持更多视频格式（不仅仅是 MP4）
5. **缓存机制**：对视频帧进行缓存，加速数据加载

---

## 风险评估

### 技术风险

1. **VQGAN 模型兼容性**：不同版本的 VQGAN 模型可能有不同的接口
   - **缓解措施**：仔细检查 VQGAN 模型的实际接口，必要时创建适配层

2. **BPP 计算准确性**：简单的均匀分布假设可能不准确
   - **缓解措施**：先实现简单方案，后续集成熵编码模型

3. **内存占用**：视频数据集可能占用更多内存
   - **缓解措施**：优化数据加载，只读取需要的帧

### 时间风险

1. **开发时间**：两个任务可能需要较长时间
   - **缓解措施**：分阶段实施，先完成数据集改造，再完成 I-frame 替换

2. **测试时间**：需要充分测试确保稳定性
   - **缓解措施**：编写自动化测试脚本

---

## 总结

本改造项目分为两个主要任务：

1. **数据集改造**：支持 MP4 视频文件，参考 `video_utils.py` 的实现
2. **I-frame 替换**：将 CompressAI 模型替换为 VQGAN 模型

两个任务都需要保持向后兼容性，确保原有功能不受影响。实施时建议分阶段进行，先完成数据集改造，再完成 I-frame 替换，每个阶段都进行充分测试。
